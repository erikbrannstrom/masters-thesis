\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{natbib}
\usepackage{graphicx}
\graphicspath{{./images/}}

\begin{document}

\title{Automated analysis and planning of online marketing campaigns}
\author{Erik Brännström\\
  Chalmers University of Technology}
\date{}
\maketitle

\section{Introduction}
Duego was founded in 2010 with the idea of creating a social networking site whose target group is those who wish to meet new
people. This stands in contrast to the existing networks that either manage people who the user already knows (e.g. Facebook)
or where the user tries to find a life partner (i.e. dating sites). Online advertisement is leveraged as a way to promote the
site to new users. This is done both using targeted ads that are adapted based on the target demographics on social networks,
as well as more conventional ads on websites. Today, managing these campaigns is a manual process that is outsourced to
another company.

The wish of Duego is to automate this process using software. Such a system would be required to analyze the existing campaigns
with regard to ads and target groups along with campaign metrics such as clicks per impression, conversion ratio, etcetera, and
use this data to suggest, or even automatically add, new campaign ads. The data set that will be used in production consists of
hundreds of different campaigns, each with a large number of ads. Along with the attributes of both campaigns and ads, this means
that the complete advertisement data set consists of hundreds of thousands of attributes to be analyzed.

\section{Problem}
This problem statement is described from the view of Duego, however it can be generalized into a broader problem in the sphere of
software engineering, namely knowledge discovery in databases (KDD) or more specifically data mining. Data mining is the process
of delegating and automating the task of identifying useful knowledge in a large set of data to a computer, either fully or
partially. The somewhat philosophical discussion of what knowledge really is and how to define what is useful knowledge will not,
however, be covered in this paper.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{mape.eps}
\caption{General MAPE control loop}
\label{fig:MAPE}
\end{figure}

Useful terminology is defined by \citet{IBM2006} in the field of autonomic computing. Figure \ref{fig:MAPE} shows an autonomic
manager, which is a component that collects data from a system and, based on this data, performs actions with the purpose of 
improving the system. This control loop is divided into four sub-tasks called monitor (collect system information), analyze 
(correlate and model data), plan (design behaviour required to reach goal) and execute (run the planned actions), sometimes 
refered to as MAPE.

In online marketing, monitoring is the collection of metrics for online advertisement. To optimize these metrics, the gathered 
information is analyzed and this analysis forms the basis for the planning of future campaigns. Once the plans are completed, the 
new campaign can be launched, with new metrics being gathered and so on.

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\textwidth]{mape-marketing.eps}
\caption{Online marketing automation system in control loop}
\label{fig:MAPEMarketing}
\end{figure}

Today only the monitoring is automated, where as the other parts of the process are performed manually. It is infeasible to fully
automate the whole process, due to for example the creative side of advertisement and the complex factors that decide which groups
an upcoming campaign should target. There are however certain areas that can be automated and this paper will focus on automated
analysis and planning of online marketing campaigns, shown in Figure \ref{fig:MAPEMarketing}.

A system to automate this process would require monitored data as input, which in this context equals historical data of campaigns
and their metrics as mentioned previously. This data would then be mined to identify subsets of campaign properties that are 
associated with a probability that an impression of that an ad with these properties leads to an action being taken by the user.
Human input is required to specify which target group the next campaign will be aimed at, and based on this the system will 
generate suggested ad properties that optimize the number of expected actions taken per impression.

\section{Scope}
Using the MAPE framework in Figure \ref{fig:MAPE}, only the analysis and planning tasks are considered part of this paper, where 
as monitoring and execution are out of scope. The latter two are however relevant in the verification step, but will not be 
covered as a research topic. In this context, this means for example that the feature of integrating this system with marketing 
services to automatically add new ads and campaigns will not be a part of the final system.

Furthermore, the data set includes attributes whose values are free text and images. Text mining and image recognition is beyond
the scope of this project. Instead these attributes will be manually categorized, so that the value space is discrete and finite.

\section{Foundations}
A number of high-level descriptions of frameworks for knowledge discovery in databases exist \citep{Fayyad1996, Frawley1992}
and they exhibit a number of commonalities. These include the importance of having a knowledgeble human operator guiding the
process in terms of supplying domain knowledge to the system formulating the goal of the knowledge discovery; feeding discovered
knowledge back into the system; and the identification and application of a discovery method, or more specifically the data mining
algorithm(s).

In data mining, the input to a system can be described using the terms \emph{concepts}, \emph{instances} and \emph{attributes},
where concept is the actual result of the mining, i.e. what we want to be learned; an instance is one single example of data to be
mined and can be compared to a row in a database; and attribute is a property of an instance, which in the database analogy would
be a column \citep{Witten2011}.

The Weka Project has defined an input format to be used for their open source data mining software called ARFF (Attribute-Relation
File Format) \citep{Garner1995, Witten2011}. This format is used by the Weka software package, but is well-defined and can be used
as the input format for custom systems as well. \citet{Witten2011} also describe how to use implementations of Weka in custom
software projects as well as how to extend the system with new functionality.

In a highly influential paper, \citet{Quinlan1986} decribe how decision trees can be created from a training set and how well it
handles the problem of unknown attributes values and noisy data. A related paper, \citet{Quinlan1987}, deal with how generated
decision trees can be simplified in order to more easily be applied. Four different methods are evaluated, one of which is the
reformulation of the tree as a set of production rules. This specific topic is further analyzed in \citet{Quinlan1987b} where such
production rules are shown to be more compact and also in many cases improve the classification of unseen data. An added positive
effect is that production rules from separate classifications can be merged more efficiently than their original decision trees.

Standard decision trees give a best-effort boolean classification of the input data, however sometimes it might be more
appropriate to give the probability that the input belongs to each of the available classes. This can be described using
probability estimation trees (PETs). \citet{Provost2003} discuss the problems of estimating these probabilities from ordinary
decision trees and goes on to show how to increase the accuracy of the estimates by performing tree pruning more conservately and
by applying the Laplace correction.  They also show that probability-bagging, meaning the combination of results from multiple
classifiers instead of just the one, greatly improves the estimates. The suggested algorithm, called C4.4, is part of a
comparative study of PETs by \citet{Chu2011} and is shown to have an impressive accuracy, though other algorithms may still be
more appropriate. A likely candidate algorithm for this thesis is the Naive Bayes tree, which is a standard decision tree with the
difference that the leaves are Naive Bayes classifiers, providing probabilities that an instance belong to each of the available
classes.

A thorough literature study on domain-specific languages is covered by \citet{Deursen2000} and covers high-level topics, such as
potential benefits and risks of using DSLs and exemplifying with a number of languages from different areas. The article also
mentions the phases of development in the creation of a new DSL, a topic which is described in more detail by \citet{Mernik2005}.
The latter article also provides insight into the relevant choices that need be made in the creation process, and both articles
help define a useful terminology for discussing the topic of domain-specific languages.

\section{Related works}
\citet{Chen1996} give an overview of the field of data mining where they mention the aspect of multi-level data mining, which
states that correlations may not commonly exist on the lowest level of granularity, but instead by forming groups of related
items. An example given would be that a specific brand of milk does not necessarily imply the purchase of a specific brand of
bread, however purchasing milk of any kind may still be correlated to the purchase of bread irrespective of brand.

A related area in data mining is association rule learning. \citet{Agrawal1993} describe how, given a large set of
transactions containing any number of different items, rules can be identified between these items. In the case of commerce,
such rules would describe how the purchase of one product would, with a probability above a certain threshold, also infer
the purchase of another product. This method could be applicable to the problem in this paper, even though it is more of a
classification problem, and some of the necessary extensions of the method are mentioned below.

\citet{Srikant1997} expands the above research by applying contraints on the items in the resulting association rules. These
constraints can either be expressed using specific items or with taxonomy rules, e.g. if an item is a descendant or ancestor
of another item. For this problem, the relation between target groups and ads can, based on existing metrics, be restructured into
a transactional table where each transaction is an impression (in the case of the impression per click metrics) so that each entry
would have an attribute stating whether or not the impression yielded a click. The item constraint would then be used to only
formulate association rules that lead to clicks.

Considering that the original market basket problem assumes binary attributes and that relational data can contain both
quantative and categorical data, \citep{Srikant1996} gives a useful description on how such values can be mapped onto the
binary (or boolean) problem space.

\citet{Hahsler2007} presents a package for the R software environment used in statistical computing, which implements a base
for transaction databases as well as integration with two of the most common mining algorithms, Apriori and Eclat. One of the
useful implementation choices is the implementation of the sparse matrix data structure, which greatly reduces memory load.

\section{Solution}
The solution has been divided into one section each for the two subtasks identified in the problem description. Analysis covers
the problem of creating an appropriate model of the available data that can be used in the next stage. Planning, in turn, is the 
utilization of that model with the purpose of outputting useful information to the operator.

\subsection{Analysis}
The natural distribution of the input data is very uneven because the number of impressions required to receive a click is high.
Consider the scenario of a click-through rate (CTR) of 1\%, which means that for every 100 impressions there would on average be one
click. The simplest possible classification rule would be a rule that for every instance gives the classification of no click, and
this would yield an accuracy of 99\%. Even though this is indeed an impressive number, it is of no use to us since it says nothing
about the instances that actually would yield clicks. This is refered to as the problem of imbalanced data sets as described by
\citet{Chawla2004}.

One way to deal with this problem is to increase the number of instances that we are interested in (positive instances) or 
decreasing the number of negative instances in so that the total distribution is even. This is called over-sampling and under-
sampling respectively, and will remove the possibility for a classifier to simply disregard the positive instances in the 
classification \citep{Chawla2004, Japkowicz2002}.

A second alternative is to use the concept of cost modification, i.e. tell the algorithm that it is more costly to misclassify
positive instances as negative instances than vice versa \citep{Chawla2004, Japkowicz2002}.

A third option is to not consider the class to be click/no click, but rather set a threshold rate for the CTR and let the top 50\%
be classified as high-performing ads and vice versa.

\section{Project plan}
The proposed workflow of this thesis is inspired by the agile practices that perhaps most notably have become common in the
software development industry. The most important methods that should be applied to this work are iterative project development
and frequent ``customer collaboration'', in this case thesis suporvisors from both Chalmers University of Technology and Duego
Technologies AB.

The project is expected to run for about 20 weeks. By dividing this relatively long period of time into iterations of four weeks,
the project will be easier to manage from my point of view as well as provide ample opportunity for the supervisors to have their
feedback incorporated.

The initial iterations will most likely require a heavy focus on research and studies of the related fields. The following
iterations will include more work on the practical side of the projects. Once the research has progressed to an extent that the
problem space as well as the approach of the solution are well defined, more effort can be put into the development of the final
application that is to be used by Duego. A preliminary list of milestones are listed below.

\begin{enumerate}
	\item Finish extended proposal (June 19)
	\item Research problem is to be well-defined and theoretical solution described (July 17)
	\item Demo a prototype of the final system (August 14)
	\item Research should be finalized and draft of final paper submitted to supervisors (September 11)
	\item Submit final paper to examinator and working system to Duego (October 9)
\end{enumerate}

\section{Verification}
The result of the thesis work would be verified by applying the results to the problem suggested by Duego in a real-world setting.
The suggested advertisement and target groups from the system will be added to their online marketing portfolio which will allow
us to analyze the effectiveness of the software. This can be done continuously over the course of the development in addition to
more traditional software unit and system testing.

\section{Thesis outline}
\begin{itemize}
	\item Abstract (0.5 pages)
	\item Introduction (0.5 pages)
	\item Problem description (1-2 pages)
	\item Related works (1 page)
	\item Solution (3-4 pages)
	\begin{itemize}
		\item Analysis step
		\item Planning step
	\end{itemize}
	\item Discussion (1 page)
	\item Conclusion (0.5 pages)
\end{itemize}

\bibliographystyle{plainnat}
\bibliography{references} % references.bib
\end{document}